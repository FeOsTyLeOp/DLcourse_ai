{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy\n",
    "import linear_classifer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\754446373.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\754446373.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "\n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "\n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])\n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "5.006760443547122"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\979710720.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\300140031.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\300140031.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\300140031.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "batch_size = 3\n",
    "predictions = np.zeros((batch_size, 3))\n",
    "target_index = np.ones(batch_size, np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\1919402437.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\1919402437.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "C:\\Users\\mcinf\\AppData\\Local\\Temp\\ipykernel_19124\\1919402437.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x18ad26b7790>]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301800\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303309\n",
      "Epoch 4, loss: 2.302126\n",
      "Epoch 5, loss: 2.301524\n",
      "Epoch 6, loss: 2.301150\n",
      "Epoch 7, loss: 2.301359\n",
      "Epoch 8, loss: 2.302305\n",
      "Epoch 9, loss: 2.302205\n",
      "Epoch 10, loss: 2.301595\n",
      "Epoch 11, loss: 2.302039\n",
      "Epoch 12, loss: 2.301247\n",
      "Epoch 13, loss: 2.301836\n",
      "Epoch 14, loss: 2.302396\n",
      "Epoch 15, loss: 2.302299\n",
      "Epoch 16, loss: 2.301161\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302180\n",
      "Epoch 19, loss: 2.302339\n",
      "Epoch 20, loss: 2.302694\n",
      "Epoch 21, loss: 2.301872\n",
      "Epoch 22, loss: 2.302261\n",
      "Epoch 23, loss: 2.302209\n",
      "Epoch 24, loss: 2.302215\n",
      "Epoch 25, loss: 2.301591\n",
      "Epoch 26, loss: 2.301535\n",
      "Epoch 27, loss: 2.302557\n",
      "Epoch 28, loss: 2.301689\n",
      "Epoch 29, loss: 2.301815\n",
      "Epoch 30, loss: 2.301483\n",
      "Epoch 31, loss: 2.301553\n",
      "Epoch 32, loss: 2.301640\n",
      "Epoch 33, loss: 2.301991\n",
      "Epoch 34, loss: 2.302104\n",
      "Epoch 35, loss: 2.301741\n",
      "Epoch 36, loss: 2.302652\n",
      "Epoch 37, loss: 2.303017\n",
      "Epoch 38, loss: 2.301521\n",
      "Epoch 39, loss: 2.301504\n",
      "Epoch 40, loss: 2.301565\n",
      "Epoch 41, loss: 2.302361\n",
      "Epoch 42, loss: 2.302223\n",
      "Epoch 43, loss: 2.302385\n",
      "Epoch 44, loss: 2.302532\n",
      "Epoch 45, loss: 2.301950\n",
      "Epoch 46, loss: 2.301686\n",
      "Epoch 47, loss: 2.301881\n",
      "Epoch 48, loss: 2.302291\n",
      "Epoch 49, loss: 2.302069\n",
      "Epoch 50, loss: 2.301971\n",
      "Epoch 51, loss: 2.302533\n",
      "Epoch 52, loss: 2.302043\n",
      "Epoch 53, loss: 2.302003\n",
      "Epoch 54, loss: 2.301729\n",
      "Epoch 55, loss: 2.301272\n",
      "Epoch 56, loss: 2.302220\n",
      "Epoch 57, loss: 2.301765\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.302229\n",
      "Epoch 60, loss: 2.301372\n",
      "Epoch 61, loss: 2.302540\n",
      "Epoch 62, loss: 2.301929\n",
      "Epoch 63, loss: 2.301502\n",
      "Epoch 64, loss: 2.302201\n",
      "Epoch 65, loss: 2.302674\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.302989\n",
      "Epoch 68, loss: 2.301388\n",
      "Epoch 69, loss: 2.301621\n",
      "Epoch 70, loss: 2.301533\n",
      "Epoch 71, loss: 2.302804\n",
      "Epoch 72, loss: 2.301514\n",
      "Epoch 73, loss: 2.300962\n",
      "Epoch 74, loss: 2.302366\n",
      "Epoch 75, loss: 2.301895\n",
      "Epoch 76, loss: 2.301768\n",
      "Epoch 77, loss: 2.302649\n",
      "Epoch 78, loss: 2.302394\n",
      "Epoch 79, loss: 2.301415\n",
      "Epoch 80, loss: 2.302148\n",
      "Epoch 81, loss: 2.302263\n",
      "Epoch 82, loss: 2.301172\n",
      "Epoch 83, loss: 2.302803\n",
      "Epoch 84, loss: 2.301892\n",
      "Epoch 85, loss: 2.302788\n",
      "Epoch 86, loss: 2.302016\n",
      "Epoch 87, loss: 2.302450\n",
      "Epoch 88, loss: 2.302002\n",
      "Epoch 89, loss: 2.302363\n",
      "Epoch 90, loss: 2.301257\n",
      "Epoch 91, loss: 2.302553\n",
      "Epoch 92, loss: 2.301951\n",
      "Epoch 93, loss: 2.300910\n",
      "Epoch 94, loss: 2.302318\n",
      "Epoch 95, loss: 2.301563\n",
      "Epoch 96, loss: 2.302494\n",
      "Epoch 97, loss: 2.301579\n",
      "Epoch 98, loss: 2.302468\n",
      "Epoch 99, loss: 2.302352\n",
      "121\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.283301\n",
      "Epoch 1, loss: 2.237674\n",
      "Epoch 2, loss: 2.218629\n",
      "Epoch 3, loss: 2.174659\n",
      "Epoch 4, loss: 2.182868\n",
      "Epoch 5, loss: 2.205173\n",
      "Epoch 6, loss: 2.125798\n",
      "Epoch 7, loss: 2.157600\n",
      "Epoch 8, loss: 2.116210\n",
      "Epoch 9, loss: 2.168020\n",
      "425\n",
      "Epoch 0, loss: 2.165254\n",
      "Epoch 1, loss: 2.223440\n",
      "Epoch 2, loss: 2.168189\n",
      "Epoch 3, loss: 2.165455\n",
      "Epoch 4, loss: 2.163593\n",
      "Epoch 5, loss: 2.070280\n",
      "Epoch 6, loss: 2.143984\n",
      "Epoch 7, loss: 2.120156\n",
      "Epoch 8, loss: 2.126146\n",
      "Epoch 9, loss: 2.108222\n",
      "449\n",
      "Epoch 0, loss: 2.153003\n",
      "Epoch 1, loss: 2.123437\n",
      "Epoch 2, loss: 2.118652\n",
      "Epoch 3, loss: 2.109143\n",
      "Epoch 4, loss: 2.114096\n",
      "Epoch 5, loss: 2.114558\n",
      "Epoch 6, loss: 2.081255\n",
      "Epoch 7, loss: 2.131835\n",
      "Epoch 8, loss: 2.120399\n",
      "Epoch 9, loss: 2.055304\n",
      "490\n",
      "Epoch 0, loss: 2.106501\n",
      "Epoch 1, loss: 2.095716\n",
      "Epoch 2, loss: 2.142633\n",
      "Epoch 3, loss: 2.090432\n",
      "Epoch 4, loss: 2.041924\n",
      "Epoch 5, loss: 2.109532\n",
      "Epoch 6, loss: 2.126855\n",
      "Epoch 7, loss: 2.098129\n",
      "Epoch 8, loss: 2.061020\n",
      "Epoch 9, loss: 2.183117\n",
      "485\n",
      "Epoch 0, loss: 2.120867\n",
      "Epoch 1, loss: 2.077816\n",
      "Epoch 2, loss: 2.159775\n",
      "Epoch 3, loss: 2.040605\n",
      "Epoch 4, loss: 2.092675\n",
      "Epoch 5, loss: 2.068102\n",
      "Epoch 6, loss: 2.071914\n",
      "Epoch 7, loss: 2.039572\n",
      "Epoch 8, loss: 2.125688\n",
      "Epoch 9, loss: 2.088801\n",
      "474\n",
      "Linear classifier with l = 1.000000e-01 and r = 1.000000e-04\n",
      "Accuracy: 0.26\n",
      "Epoch 0, loss: 2.268891\n",
      "Epoch 1, loss: 2.226491\n",
      "Epoch 2, loss: 2.223558\n",
      "Epoch 3, loss: 2.219854\n",
      "Epoch 4, loss: 2.186610\n",
      "Epoch 5, loss: 2.193292\n",
      "Epoch 6, loss: 2.142044\n",
      "Epoch 7, loss: 2.127206\n",
      "Epoch 8, loss: 2.155070\n",
      "Epoch 9, loss: 2.140613\n",
      "395\n",
      "Epoch 0, loss: 2.112074\n",
      "Epoch 1, loss: 2.148192\n",
      "Epoch 2, loss: 2.158573\n",
      "Epoch 3, loss: 2.122455\n",
      "Epoch 4, loss: 2.123997\n",
      "Epoch 5, loss: 2.124256\n",
      "Epoch 6, loss: 2.056661\n",
      "Epoch 7, loss: 2.110823\n",
      "Epoch 8, loss: 2.154789\n",
      "Epoch 9, loss: 2.232233\n",
      "417\n",
      "Epoch 0, loss: 2.176932\n",
      "Epoch 1, loss: 2.132491\n",
      "Epoch 2, loss: 2.142340\n",
      "Epoch 3, loss: 2.134956\n",
      "Epoch 4, loss: 2.106450\n",
      "Epoch 5, loss: 2.088427\n",
      "Epoch 6, loss: 2.158740\n",
      "Epoch 7, loss: 2.085661\n",
      "Epoch 8, loss: 2.051602\n",
      "Epoch 9, loss: 2.087251\n",
      "488\n",
      "Epoch 0, loss: 2.089434\n",
      "Epoch 1, loss: 2.183734\n",
      "Epoch 2, loss: 2.072665\n",
      "Epoch 3, loss: 2.163433\n",
      "Epoch 4, loss: 2.108861\n",
      "Epoch 5, loss: 2.120713\n",
      "Epoch 6, loss: 2.139802\n",
      "Epoch 7, loss: 2.128352\n",
      "Epoch 8, loss: 2.114310\n",
      "Epoch 9, loss: 2.133108\n",
      "485\n",
      "Epoch 0, loss: 2.082440\n",
      "Epoch 1, loss: 2.136528\n",
      "Epoch 2, loss: 2.064530\n",
      "Epoch 3, loss: 2.079715\n",
      "Epoch 4, loss: 2.048867\n",
      "Epoch 5, loss: 2.119671\n",
      "Epoch 6, loss: 2.106832\n",
      "Epoch 7, loss: 2.122857\n",
      "Epoch 8, loss: 2.167909\n",
      "Epoch 9, loss: 2.112636\n",
      "468\n",
      "Linear classifier with l = 1.000000e-01 and r = 1.000000e-05\n",
      "Accuracy: 0.25\n",
      "Epoch 0, loss: 2.296338\n",
      "Epoch 1, loss: 2.292394\n",
      "Epoch 2, loss: 2.283120\n",
      "Epoch 3, loss: 2.272819\n",
      "Epoch 4, loss: 2.277282\n",
      "Epoch 5, loss: 2.274309\n",
      "Epoch 6, loss: 2.256156\n",
      "Epoch 7, loss: 2.252585\n",
      "Epoch 8, loss: 2.253079\n",
      "Epoch 9, loss: 2.239022\n",
      "349\n",
      "Epoch 0, loss: 2.242918\n",
      "Epoch 1, loss: 2.250413\n",
      "Epoch 2, loss: 2.247569\n",
      "Epoch 3, loss: 2.237777\n",
      "Epoch 4, loss: 2.206485\n",
      "Epoch 5, loss: 2.233632\n",
      "Epoch 6, loss: 2.200767\n",
      "Epoch 7, loss: 2.217042\n",
      "Epoch 8, loss: 2.197131\n",
      "Epoch 9, loss: 2.204682\n",
      "385\n",
      "Epoch 0, loss: 2.197073\n",
      "Epoch 1, loss: 2.179370\n",
      "Epoch 2, loss: 2.201697\n",
      "Epoch 3, loss: 2.206961\n",
      "Epoch 4, loss: 2.220110\n",
      "Epoch 5, loss: 2.202971\n",
      "Epoch 6, loss: 2.203290\n",
      "Epoch 7, loss: 2.241675\n",
      "Epoch 8, loss: 2.204677\n",
      "Epoch 9, loss: 2.154343\n",
      "449\n",
      "Epoch 0, loss: 2.181830\n",
      "Epoch 1, loss: 2.146571\n",
      "Epoch 2, loss: 2.198394\n",
      "Epoch 3, loss: 2.185385\n",
      "Epoch 4, loss: 2.151443\n",
      "Epoch 5, loss: 2.210143\n",
      "Epoch 6, loss: 2.195171\n",
      "Epoch 7, loss: 2.183873\n",
      "Epoch 8, loss: 2.190450\n",
      "Epoch 9, loss: 2.153226\n",
      "429\n",
      "Epoch 0, loss: 2.159828\n",
      "Epoch 1, loss: 2.179908\n",
      "Epoch 2, loss: 2.163325\n",
      "Epoch 3, loss: 2.170326\n",
      "Epoch 4, loss: 2.184971\n",
      "Epoch 5, loss: 2.183053\n",
      "Epoch 6, loss: 2.198865\n",
      "Epoch 7, loss: 2.196950\n",
      "Epoch 8, loss: 2.167963\n",
      "Epoch 9, loss: 2.209829\n",
      "411\n",
      "Linear classifier with l = 1.000000e-02 and r = 1.000000e-04\n",
      "Accuracy: 0.22\n",
      "Epoch 0, loss: 2.300446\n",
      "Epoch 1, loss: 2.286729\n",
      "Epoch 2, loss: 2.287880\n",
      "Epoch 3, loss: 2.268093\n",
      "Epoch 4, loss: 2.278513\n",
      "Epoch 5, loss: 2.258032\n",
      "Epoch 6, loss: 2.249330\n",
      "Epoch 7, loss: 2.262828\n",
      "Epoch 8, loss: 2.250885\n",
      "Epoch 9, loss: 2.245673\n",
      "334\n",
      "Epoch 0, loss: 2.243871\n",
      "Epoch 1, loss: 2.250227\n",
      "Epoch 2, loss: 2.257353\n",
      "Epoch 3, loss: 2.209028\n",
      "Epoch 4, loss: 2.215732\n",
      "Epoch 5, loss: 2.215000\n",
      "Epoch 6, loss: 2.207022\n",
      "Epoch 7, loss: 2.226611\n",
      "Epoch 8, loss: 2.205435\n",
      "Epoch 9, loss: 2.229269\n",
      "390\n",
      "Epoch 0, loss: 2.210988\n",
      "Epoch 1, loss: 2.229440\n",
      "Epoch 2, loss: 2.204743\n",
      "Epoch 3, loss: 2.216860\n",
      "Epoch 4, loss: 2.219766\n",
      "Epoch 5, loss: 2.221555\n",
      "Epoch 6, loss: 2.174166\n",
      "Epoch 7, loss: 2.215800\n",
      "Epoch 8, loss: 2.156984\n",
      "Epoch 9, loss: 2.201833\n",
      "444\n",
      "Epoch 0, loss: 2.192184\n",
      "Epoch 1, loss: 2.203247\n",
      "Epoch 2, loss: 2.191922\n",
      "Epoch 3, loss: 2.172915\n",
      "Epoch 4, loss: 2.173206\n",
      "Epoch 5, loss: 2.190169\n",
      "Epoch 6, loss: 2.193358\n",
      "Epoch 7, loss: 2.173061\n",
      "Epoch 8, loss: 2.183115\n",
      "Epoch 9, loss: 2.139392\n",
      "434\n",
      "Epoch 0, loss: 2.174707\n",
      "Epoch 1, loss: 2.174463\n",
      "Epoch 2, loss: 2.206091\n",
      "Epoch 3, loss: 2.159055\n",
      "Epoch 4, loss: 2.173167\n",
      "Epoch 5, loss: 2.211633\n",
      "Epoch 6, loss: 2.149038\n",
      "Epoch 7, loss: 2.174428\n",
      "Epoch 8, loss: 2.171251\n",
      "Epoch 9, loss: 2.155407\n",
      "406\n",
      "Linear classifier with l = 1.000000e-02 and r = 1.000000e-05\n",
      "Accuracy: 0.22\n",
      "Epoch 0, loss: 2.302453\n",
      "Epoch 1, loss: 2.300103\n",
      "Epoch 2, loss: 2.299960\n",
      "Epoch 3, loss: 2.301491\n",
      "Epoch 4, loss: 2.299463\n",
      "Epoch 5, loss: 2.298042\n",
      "Epoch 6, loss: 2.294969\n",
      "Epoch 7, loss: 2.296028\n",
      "Epoch 8, loss: 2.292388\n",
      "Epoch 9, loss: 2.297074\n",
      "253\n",
      "Epoch 0, loss: 2.293402\n",
      "Epoch 1, loss: 2.294862\n",
      "Epoch 2, loss: 2.290480\n",
      "Epoch 3, loss: 2.292851\n",
      "Epoch 4, loss: 2.286883\n",
      "Epoch 5, loss: 2.291848\n",
      "Epoch 6, loss: 2.295411\n",
      "Epoch 7, loss: 2.289443\n",
      "Epoch 8, loss: 2.284183\n",
      "Epoch 9, loss: 2.284345\n",
      "301\n",
      "Epoch 0, loss: 2.291227\n",
      "Epoch 1, loss: 2.285177\n",
      "Epoch 2, loss: 2.287568\n",
      "Epoch 3, loss: 2.286061\n",
      "Epoch 4, loss: 2.279198\n",
      "Epoch 5, loss: 2.289465\n",
      "Epoch 6, loss: 2.292042\n",
      "Epoch 7, loss: 2.277723\n",
      "Epoch 8, loss: 2.273004\n",
      "Epoch 9, loss: 2.287836\n",
      "382\n",
      "Epoch 0, loss: 2.284193\n",
      "Epoch 1, loss: 2.281179\n",
      "Epoch 2, loss: 2.276180\n",
      "Epoch 3, loss: 2.281648\n",
      "Epoch 4, loss: 2.273985\n",
      "Epoch 5, loss: 2.281001\n",
      "Epoch 6, loss: 2.286058\n",
      "Epoch 7, loss: 2.276957\n",
      "Epoch 8, loss: 2.276428\n",
      "Epoch 9, loss: 2.279310\n",
      "402\n",
      "Epoch 0, loss: 2.279253\n",
      "Epoch 1, loss: 2.280927\n",
      "Epoch 2, loss: 2.276110\n",
      "Epoch 3, loss: 2.280283\n",
      "Epoch 4, loss: 2.280153\n",
      "Epoch 5, loss: 2.275304\n",
      "Epoch 6, loss: 2.273921\n",
      "Epoch 7, loss: 2.273512\n",
      "Epoch 8, loss: 2.283989\n",
      "Epoch 9, loss: 2.274351\n",
      "417\n",
      "Linear classifier with l = 1.000000e-03 and r = 1.000000e-04\n",
      "Accuracy: 0.20\n",
      "Epoch 0, loss: 2.302070\n",
      "Epoch 1, loss: 2.302097\n",
      "Epoch 2, loss: 2.300489\n",
      "Epoch 3, loss: 2.300245\n",
      "Epoch 4, loss: 2.297231\n",
      "Epoch 5, loss: 2.297874\n",
      "Epoch 6, loss: 2.296012\n",
      "Epoch 7, loss: 2.294948\n",
      "Epoch 8, loss: 2.294004\n",
      "Epoch 9, loss: 2.294900\n",
      "254\n",
      "Epoch 0, loss: 2.293411\n",
      "Epoch 1, loss: 2.292357\n",
      "Epoch 2, loss: 2.294771\n",
      "Epoch 3, loss: 2.292557\n",
      "Epoch 4, loss: 2.294838\n",
      "Epoch 5, loss: 2.290694\n",
      "Epoch 6, loss: 2.288732\n",
      "Epoch 7, loss: 2.283669\n",
      "Epoch 8, loss: 2.291367\n",
      "Epoch 9, loss: 2.291116\n",
      "297\n",
      "Epoch 0, loss: 2.286009\n",
      "Epoch 1, loss: 2.291838\n",
      "Epoch 2, loss: 2.287452\n",
      "Epoch 3, loss: 2.287686\n",
      "Epoch 4, loss: 2.281907\n",
      "Epoch 5, loss: 2.283982\n",
      "Epoch 6, loss: 2.290031\n",
      "Epoch 7, loss: 2.287461\n",
      "Epoch 8, loss: 2.284402\n",
      "Epoch 9, loss: 2.277147\n",
      "376\n",
      "Epoch 0, loss: 2.281237\n",
      "Epoch 1, loss: 2.281991\n",
      "Epoch 2, loss: 2.281334\n",
      "Epoch 3, loss: 2.275222\n",
      "Epoch 4, loss: 2.282322\n",
      "Epoch 5, loss: 2.280616\n",
      "Epoch 6, loss: 2.275595\n",
      "Epoch 7, loss: 2.276845\n",
      "Epoch 8, loss: 2.276869\n",
      "Epoch 9, loss: 2.278167\n",
      "400\n",
      "Epoch 0, loss: 2.274619\n",
      "Epoch 1, loss: 2.278330\n",
      "Epoch 2, loss: 2.278649\n",
      "Epoch 3, loss: 2.271017\n",
      "Epoch 4, loss: 2.275027\n",
      "Epoch 5, loss: 2.283986\n",
      "Epoch 6, loss: 2.269514\n",
      "Epoch 7, loss: 2.266518\n",
      "Epoch 8, loss: 2.258166\n",
      "Epoch 9, loss: 2.280680\n",
      "414\n",
      "Linear classifier with l = 1.000000e-03 and r = 1.000000e-05\n",
      "Accuracy: 0.19\n",
      "l, r = 0.001, 0.0001, accuracy = 0.195000\n",
      "l, r = 0.001, 1e-05, accuracy = 0.193444\n",
      "l, r = 0.01, 0.0001, accuracy = 0.224778\n",
      "l, r = 0.01, 1e-05, accuracy = 0.223111\n",
      "l, r = 0.1, 0.0001, accuracy = 0.258111\n",
      "l, r = 0.1, 1e-05, accuracy = 0.250333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 300\n",
    "\n",
    "#learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "#reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "learning_rates = [1e-1, 1e-2, 1e-3]\n",
    "reg_strengths = [1e-4, 1e-5]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength\n",
    "# than provided initially\n",
    "\n",
    "# Find the best k using cross-validation based on accuracy\n",
    "num_folds = 5\n",
    "train_folds_X = []\n",
    "test_folds_X = []\n",
    "l_r_to_accuracy = {}\n",
    "\n",
    "# TODO: split the training data in 5 folds and store them in train_folds_X/train_folds_y\n",
    "\n",
    "kfold = KFold(num_folds)\n",
    "\n",
    "for train, test in kfold.split(train_X):\n",
    "    train_folds_X.append(train)\n",
    "    test_folds_X.append(test)\n",
    "\n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "        # TODO: perform cross-validation\n",
    "        # Go through every fold and use it for testing and all other folds for validation\n",
    "        # Perform training and produce accuracy metric on the validation dataset\n",
    "        # Average accuracy from all the folds and write it into k_to_accuracy\n",
    "        accuracy_all = []\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        for i, train in enumerate(train_folds_X):\n",
    "            loss_history = classifier.fit(train_X[train], train_y[train], epochs=num_epochs, learning_rate=l, batch_size=batch_size, reg=r)\n",
    "            pred = classifier.predict(train_X[test_folds_X[i]])\n",
    "            accuracy = multiclass_accuracy(pred, train_y[test_folds_X[i]])\n",
    "            accuracy_all.append(accuracy)\n",
    "\n",
    "        l_r_to_accuracy[str(l) + ', ' + str(r)] = np.average(accuracy_all)\n",
    "        print(\"Linear classifier with l = %e and r = %e\" % (l, r))\n",
    "        print(\"Accuracy: %4.2f\" % (l_r_to_accuracy[str(l) + ', ' + str(r)]))\n",
    "\n",
    "for lr in sorted(l_r_to_accuracy):\n",
    "    print('l, r = %s, accuracy = %f' % (lr, l_r_to_accuracy[lr]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
