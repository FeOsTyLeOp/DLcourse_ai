{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Intstall PyTorch and download data\n",
    "!pip3 install torch torchvision\n",
    "\n",
    "!wget -c http: // ufldl.stanford.edu / housenumbers / train_32x32.mat http: // ufldl.stanford.edu / housenumbers / test_32x32.mat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import itertools\n",
    "\n",
    "from torchvision import transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, lets load the dataset\n",
    "data_train = dset.SVHN('./',\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])\n",
    "                       ]),download = True)\n",
    "\n",
    "data_test = dset.SVHN('./', split='test', transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])\n",
    "                       ]),download = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "data_size = data_train.data.shape[0]\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We'll use a special helper module to shape it into a flat tensor\n",
    "class Flattener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.view(batch_size, -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a simple network with new layers:\n",
    "\n",
    "Convolutional - nn.Conv2d\n",
    "\n",
    "MaxPool - nn.MaxPool2d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),\n",
    "            Flattener(),\n",
    "            nn.Linear(64*2*2, 10),\n",
    "          )\n",
    "\n",
    "nn_model.type(torch.cuda.FloatTensor)\n",
    "nn_model.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-1, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler = StepLR(optimizer, step_size = 1, gamma = 1.0)):\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        model.train() # Enter train mode\n",
    "\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)\n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            indices = torch.argmax(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "\n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "\n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "\n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        print(f\"Device: {next(model.parameters()).device}\")\n",
    "\n",
    "    return loss_history, train_history, val_history\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "\n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    # TODO: Copy implementation from previous assignment\n",
    "    # Don't forget to move the data to device before running it through the model!\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(loader):\n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)\n",
    "            indices = torch.argmax(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "\n",
    "        val_accuracy = float(correct_samples) / total_samples\n",
    "    return val_accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We use the following generation algorithms below:\n",
    "\n",
    "ColorJitter - random color change\n",
    "\n",
    "RandomHorizontalFlip - horizontal reflection with 50% probability\n",
    "\n",
    "RandomVerticalFlip - vertical reflection with 50% probability\n",
    "\n",
    "RandomRotation - random rotation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfs = transforms.Compose([\n",
    "    transforms.ColorJitter(hue=.50, saturation=.50),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(50, resample=PIL.Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                       std=[0.20,0.20,0.20])\n",
    "])\n",
    "\n",
    "# Create augmented train dataset\n",
    "data_aug_train = dset.SVHN('./',\n",
    "                       transform=tfs\n",
    "                      )\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(data_aug_train, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfs = transforms.Compose([\n",
    "    #transforms.ColorJitter(hue=.20, saturation=.20),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
    "])\n",
    "\n",
    "data_aug_vis = dset.SVHN('./',\n",
    "                       transform=tfs\n",
    "                      )\n",
    "\n",
    "plt.figure(figsize=(30, 3))\n",
    "\n",
    "for i, (x, y) in enumerate(data_aug_vis):\n",
    "    if i == 10:\n",
    "        break\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x)\n",
    "    plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO:\n",
    "tfs = transforms.Compose([\n",
    "    # TODO: Add good augmentations\n",
    "    transforms.ColorJitter(hue=.20, saturation=.20),\n",
    "    transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                       std=[0.20,0.20,0.20])\n",
    "])\n",
    "\n",
    "# TODO create new instances of loaders with the augmentations you chose\n",
    "data_aug_train = dset.SVHN('./',\n",
    "                       transform=tfs\n",
    "                      )\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(data_aug_train, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, let's train with augmentations!\n",
    "\n",
    "# Note we shouldn't use augmentations on validation\n",
    "\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_aug_loader, val_loader, loss, optimizer, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LeNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Implement LeNet-like architecture for SVHN task\n",
    "lenet_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Flattener(),\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(84, 10)\n",
    "          )\n",
    "\n",
    "lenet_model.type(torch.cuda.FloatTensor)\n",
    "lenet_model.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.SGD(lenet_model.parameters(), lr=1e-1, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's train it!\n",
    "loss_history, train_history, val_history = train_model(lenet_model, train_aug_loader, val_loader, loss, optimizer, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The key hyperparameters we're going to tune are learning speed, annealing rate and regularization\n",
    "# We also encourage you to try different optimizers as well\n",
    "\n",
    "Hyperparams_t = namedtuple(\"Hyperparams_t\", ['learning_rate', 'anneal_epochs', 'reg'])\n",
    "RunResult_t = namedtuple(\"RunResult_t\", ['model', 'train_history', 'val_history', 'final_val_accuracy'])\n",
    "\n",
    "learning_rates = [1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "anneal_coeff = 0.2\n",
    "anneal_epochs = [1, 5, 10, 15, 20, 50]\n",
    "reg = [1e-3, 1e-4, 1e-5, 1e-7]\n",
    "\n",
    "parameters = list(itertools.product(learning_rates, anneal_epochs, reg))\n",
    "\n",
    "batch_size = 64\n",
    "epoch_num = 10\n",
    "\n",
    "# Record all the runs here\n",
    "# Key should be Hyperparams and values should be RunResult\n",
    "run_record = {}\n",
    "\n",
    "# Use grid search or random search and record all runs in run_record dictionnary\n",
    "# Important: perform search in logarithmic space!\n",
    "\n",
    "# TODO: Your code here!\n",
    "for j, (learning_rate, anneal_epochs, reg) in enumerate(parameters):\n",
    "  Hyperparams = Hyperparams_t(learning_rate, anneal_epochs, reg)\n",
    "  lenet_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Flattener(),\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(84, 10)\n",
    "          )\n",
    "  lenet_model.type(torch.cuda.FloatTensor)\n",
    "  if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    lenet_model = nn.DataParallel(lenet_model)\n",
    "  lenet_model.to(device)\n",
    "  loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "\n",
    "  optimizer = optim.SGD(lenet_model.parameters(), lr=Hyperparams.learning_rate, weight_decay = Hyperparams.reg)\n",
    "  scheduler = StepLR(optimizer, step_size = Hyperparams.anneal_epochs, gamma = anneal_coeff)\n",
    "  loss_history, train_history, val_history = train_model(lenet_model, train_aug_loader, val_loader, loss, optimizer, epoch_num, scheduler)\n",
    "  run_record[Hyperparams] = RunResult_t(lenet_model.copy(), train_history.copy(), val_history.copy(), val_history[-1].copy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_val_accuracy = None\n",
    "best_hyperparams = None\n",
    "best_run = None\n",
    "\n",
    "for hyperparams, run_result in run_record.items():\n",
    "    if best_val_accuracy is None or best_val_accuracy < run_result.final_val_accuracy:\n",
    "        best_val_accuracy = run_result.final_val_accuracy\n",
    "        best_hyperparams = hyperparams\n",
    "        best_run = run_result\n",
    "\n",
    "print(\"Best validation accuracy: %4.2f, best hyperparams: %s\" % (best_val_accuracy, best_hyperparams))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),\n",
    "            Flattener(),\n",
    "            nn.Linear(128*2*2, 10),\n",
    "\n",
    "          )\n",
    "\n",
    "best_model.type(torch.cuda.FloatTensor)\n",
    "best_model.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
    "scheduler = StepLR(optimizer, step_size = 5, gamma=0.5)\n",
    "loss_history, train_history, val_history = train_model(best_model, train_aug_loader, val_loader, loss, optimizer, 20, scheduler);"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
